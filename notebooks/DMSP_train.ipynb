{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMSP_train.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "09G3aT0Vyf7u"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nok191ALytan"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c6gYF050YoA"
      },
      "source": [
        "!pip install spectral\n",
        "!pip install -q -U tensorflow-addons\n",
        "!pip install scikit-image --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNrJGUdX0IYb"
      },
      "source": [
        "import spectral.io.envi as envi\n",
        "from spectral import *\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDRV2EszaHOT"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "from skimage import filters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unpx4KqqyuUf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Input, Dropout, Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D, ZeroPadding2D\n",
        "from tensorflow.keras.layers import Concatenate, Add, Activation, Multiply\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.python.keras.preprocessing import dataset_utils\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.activations import relu\n",
        "import numpy as np\n",
        "from tifffile import imshow, imread"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiANsyeOzvTK"
      },
      "source": [
        "img2 = envi.open('/content/drive/MyDrive/Diploma/data_new/F15_20160101_20161231.v4.stable_lights.shiftx1y1.avg_vis.hdr', '/content/drive/MyDrive/Diploma/data_new/F15_20160101_20161231.v4.stable_lights.shiftx1y1.avg_vis')\n",
        "img1 = envi.open('/content/drive/MyDrive/Diploma/data_new/VNL_v2_npp_2016_global_vcmslcfg_c202102150000.average_masked_resampled.hdr', '/content/drive/MyDrive/Diploma/data_new/VNL_v2_npp_2016_global_vcmslcfg_c202102150000.average_masked_resampled')\n",
        "x_train = img1.load()\n",
        "y_train = img2.load()\n",
        "clip_value = 2000\n",
        "x_train = np.clip(x_train, 0, clip_value)\n",
        "x_train /= x_train.max()\n",
        "y_train /= y_train.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYrwirKc3YEC"
      },
      "source": [
        "def get_generator(x_train, y_train, quantile=0.001, patch_size=64, count=10000):\n",
        "  \n",
        "  mask = x_train > np.quantile(x_train, quantile)\n",
        "  coords = np.array(np.where(mask)).T\n",
        "  rng = np.random.default_rng()\n",
        "  numbers = rng.choice(coords, size=count, replace=False)\n",
        "  def generate_data_x():\n",
        "    for c in numbers:\n",
        "      patch = x_train[c[0] - patch_size: c[0], c[1] - patch_size: c[1]]\n",
        "      if patch.shape == (patch_size, patch_size, 1):\n",
        "        yield patch\n",
        "  def generate_data_y():\n",
        "    for c in numbers:\n",
        "      patch =  y_train[c[0] - patch_size: c[0], c[1] - patch_size: c[1]]\n",
        "      if patch.shape == (patch_size, patch_size, 1):\n",
        "        yield patch\n",
        "    \n",
        "  return generate_data_x, generate_data_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdWuBLne_wfB"
      },
      "source": [
        "# for i, j in zip(gen[0](), gen[1]()):\n",
        "#   imshow(i)\n",
        "#   imshow(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv8uZOSWChgY"
      },
      "source": [
        "def conv_block(n_filter, kernel_size,\n",
        "               activation='relu',\n",
        "               normalization=True,\n",
        "               padding='same',\n",
        "               dropout=0.0,\n",
        "               num_blocks=2,\n",
        "               residual=True\n",
        "               ):\n",
        "  \n",
        "  def cnv(s):\n",
        "    if normalization:\n",
        "      s = tfa.layers.InstanceNormalization(\n",
        "          axis=-1,\n",
        "          center=True, \n",
        "          scale=True,\n",
        "          beta_initializer=\"random_uniform\",\n",
        "          gamma_initializer=\"random_uniform\"\n",
        "          )(s)\n",
        "    s = Activation(activation)(s)\n",
        "    s = Conv2D(n_filter, kernel_size, padding=padding)(s)\n",
        "    if dropout > 0.0:\n",
        "      s = Dropout(dropout, )(s)\n",
        "    return s\n",
        "\n",
        "  def blocks(s):\n",
        "    if residual:\n",
        "      res = s\n",
        "    for i in range(num_blocks):\n",
        "      s = cnv(s)\n",
        "    if residual:\n",
        "      if res.shape[-1] != s.shape[-1]:\n",
        "        res = Conv2D(s.shape[-1], (1,1))(res)\n",
        "      s = tf.keras.layers.Add()([res, s])\n",
        "    return s\n",
        "\n",
        "  return blocks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ8rC2SCdaXg"
      },
      "source": [
        "def unet(input_shape,\n",
        "         last_activation=tf.keras.layers.ReLU(),\n",
        "         n_depth=2,\n",
        "         n_filter_base=16,\n",
        "         kernel_size=(3,3),\n",
        "         n_conv_per_depth=2,\n",
        "         activation='relu',\n",
        "         normalization=True,\n",
        "         dropout=0.0,\n",
        "         pool_size=(2,2),\n",
        "         num_blocks=2\n",
        "         ):\n",
        "\n",
        "  input = Input(input_shape, dtype=tf.float32)\n",
        "  layer = input\n",
        "\n",
        "  skip_layers = []\n",
        "  # print(layer.shape)\n",
        "  # down\n",
        "  for n in range(n_depth):\n",
        "    for i in range(n_conv_per_depth):\n",
        "      layer = conv_block(\n",
        "          n_filter_base * 2 ** n,\n",
        "          kernel_size,\n",
        "          activation=activation,\n",
        "          normalization=normalization,\n",
        "          num_blocks=num_blocks,\n",
        "          dropout=dropout,\n",
        "          padding='same'\n",
        "          )(layer)\n",
        "    skip_layers.append(layer)\n",
        "    layer = MaxPool2D(pool_size)(layer)\n",
        "\n",
        "  # middle\n",
        "  for i in range(n_conv_per_depth - 1):\n",
        "    # print(layer.shape)\n",
        "    layer = conv_block(\n",
        "        n_filter_base * 2 ** n_depth,\n",
        "        kernel_size, activation=activation,\n",
        "        normalization=normalization,\n",
        "        num_blocks=num_blocks,\n",
        "        dropout=dropout,\n",
        "        padding='same'\n",
        "        )(layer)\n",
        "  # print(layer.shape)\n",
        "  layer = conv_block(\n",
        "      n_filter_base * 2 ** max(n_depth - 1, 0),\n",
        "      kernel_size,\n",
        "      activation=activation,\n",
        "      normalization=normalization,\n",
        "      num_blocks=num_blocks,\n",
        "      dropout=dropout,\n",
        "      padding='same'\n",
        "      )(layer)\n",
        "  # print(layer.shape)\n",
        "  # up\n",
        "  for n in range(n_depth - 1, -1, -1):\n",
        "    up = UpSampling2D(pool_size)(layer)\n",
        "    layer = Concatenate()([up, skip_layers[n]])\n",
        "    for i in range(n_conv_per_depth - 1):\n",
        "      layer = conv_block(\n",
        "        n_filter_base * 2 ** n,\n",
        "        kernel_size,\n",
        "        activation=activation,\n",
        "        normalization=normalization,\n",
        "        num_blocks=num_blocks,\n",
        "        dropout=dropout,\n",
        "        padding='same'\n",
        "        )(layer)\n",
        "    # print(layer.shape)\n",
        "    layer = conv_block(\n",
        "        n_filter_base * 2 ** max(n-1, 0),\n",
        "        kernel_size,\n",
        "        activation=activation,\n",
        "        normalization=normalization,\n",
        "        num_blocks=num_blocks,\n",
        "        dropout=dropout,\n",
        "        padding='same'\n",
        "        )(layer)    \n",
        "  output = conv_block(\n",
        "    input_shape[-1],\n",
        "    (1,) * len(kernel_size),\n",
        "    activation='linear',\n",
        "    normalization=normalization,\n",
        "    num_blocks=1,\n",
        "    padding='same',\n",
        "    residual=False\n",
        "    )(layer)\n",
        "  # output = Add()([output, input])\n",
        "  output = last_activation(output)\n",
        "\n",
        "  return Model(inputs=input, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iArW7z9AQZan"
      },
      "source": [
        "def apply_preprocessing(x, y):\n",
        "  # (x, y) = data\n",
        "  x1 = tf.image.rot90(x)\n",
        "  y1 = tf.image.rot90(y)\n",
        "  x2 = tf.image.rot90(x1)\n",
        "  y2 = tf.image.rot90(y1)\n",
        "  x3 = tf.image.rot90(x2)\n",
        "  y3 = tf.image.rot90(y2)\n",
        "  x4 = tf.image.flip_left_right(x)\n",
        "  y4 = tf.image.flip_left_right(y)\n",
        "  x5 = tf.image.flip_up_down(x)\n",
        "  y5 = tf.image.flip_up_down(y)\n",
        "  return tf.data.Dataset.from_tensors((tf.convert_to_tensor((x,x1,x2,x3,x4,x5)), tf.convert_to_tensor((y, y1,y2,y3,y4,y5))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki_KCXkMLBZC"
      },
      "source": [
        "def train(    \n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size = 5,\n",
        "    train_size = 100,\n",
        "    epochs = 1,\n",
        "    quant = 0.001,\n",
        "    patch_size = 256,\n",
        "    loss_func = tf.keras.losses.MeanAbsoluteError(),\n",
        "    lr=0.002,\n",
        "    lowest_lr=0.0002\n",
        "    ):  \n",
        "\n",
        "\n",
        "  metric = tf.keras.metrics.MeanAbsoluteError()\n",
        "\n",
        "  @tf.function\n",
        "  def step(x, y_true, optimizer):\n",
        "      with tf.GradientTape() as tape:\n",
        "        y_pred = model(x)\n",
        "        loss = loss_func(y_true, y_pred)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      metric.update_state(y_true, y_pred)\n",
        "  \n",
        "  # tf.profiler.experimental.start(log_dir)\n",
        "  decay = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "      lr,\n",
        "      int(train_size * epochs * 6 / batch_size),\n",
        "      lowest_lr / lr,\n",
        "      staircase=False\n",
        "      )\n",
        "  optimizer = tf.optimizers.Adam(decay)\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "\n",
        "\n",
        "    gen_x, gen_y = get_generator(x_train, y_train, quant, patch_size=patch_size, count=train_size)\n",
        "    x_train_dataset = tf.data.Dataset.from_generator(gen_x, output_signature=tf.TensorSpec(shape=(patch_size, patch_size, 1), dtype=tf.float32))\n",
        "    y_train_dataset = tf.data.Dataset.from_generator(gen_y, output_signature=tf.TensorSpec(shape=(patch_size, patch_size, 1), dtype=tf.float32))\n",
        "    train_dataset = tf.data.Dataset.zip((x_train_dataset, y_train_dataset)).flat_map(apply_preprocessing).unbatch().shuffle(10000).batch(batch_size)\n",
        "    for x, y_true in train_dataset:\n",
        "      step(x, y_true, optimizer)\n",
        "    # for x, y_true in test_dataset:\n",
        "    #   y_pred = model(x)\n",
        "    #   loss = tf.keras.losses.MeanAbsoluteError()(y_true, y_pred)\n",
        "    print(' train_loss = {:.5f}, cur_lr = {:.5f}'.format(metric.result(), optimizer._decayed_lr(tf.float32).numpy()))\n",
        "    metric.reset_states()\n",
        "    # if epoch % 5 == 0:\n",
        "    #   model.save_weights('/content/drive/MyDrive/Diploma/my_models/model_custom_loss_depth_3_balanced_ckpt_epoch{}'.format(epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikT0mGyTf1Mw"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import datetime\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_images=True, update_freq=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd8PPevceWuB"
      },
      "source": [
        "model = unet((256, 256, 1), last_activation=tf.keras.layers.ReLU(1.), n_depth=4, n_filter_base=32, kernel_size=(3,3), n_conv_per_depth=3, dropout=0.2)\n",
        "tensorboard_callback.set_model(model)\n",
        "model.summary()\n",
        "model.load_weights('/content/drive/MyDrive/Diploma/my_models/cur_model_epoch0.h5')\n",
        "# model = tf.keras.models.load_model('/content/drive/MyDrive/Diploma/my_models/model_custom_loss.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ma3IBvI-aMy"
      },
      "source": [
        "def custom_loss(power=5/4):\n",
        "  def my_loss(Y_true, Y_pred):\n",
        "    return tf.abs(tf.reduce_mean(tf.pow(Y_true, power) - tf.pow(Y_pred, power), axis=-1))\n",
        "  return my_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL8yKENFf5s1"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYAlpkBWymxw"
      },
      "source": [
        "def perform_training(x_test, y_test, total=10, min_quantile=0.001, max_quantile=0.5, test_size=1024):\n",
        "  patch_size = 256\n",
        "  gen_x, gen_y = get_generator(x_test, y_test, 0.001, patch_size=patch_size, count=test_size)\n",
        "  x_test_dataset = tf.data.Dataset.from_generator(gen_x, output_signature=tf.TensorSpec(shape=(patch_size, patch_size, 1), dtype=tf.float32))\n",
        "  y_test_dataset = tf.data.Dataset.from_generator(gen_y, output_signature=tf.TensorSpec(shape=(patch_size, patch_size, 1), dtype=tf.float32))\n",
        "  test_dataset = tf.data.Dataset.zip((x_test_dataset, y_test_dataset)).batch(24)\n",
        "  metric = tf.keras.metrics.MeanAbsoluteError()\n",
        "  # sup_model = tf.keras.models.clone_model(model)\n",
        "  \n",
        "  # print(sup_model.layers[3].get_weights())\n",
        "  # for layer in sup_model.layers:\n",
        "  #   w_and_b = layer.get_weights()\n",
        "  #   w_and_b_new = []\n",
        "  #   for w in w_and_b:\n",
        "  #     w_and_b_new.append(np.zeros_like(w))\n",
        "  #   layer.set_weights(w_and_b_new)\n",
        "  # print(sup_model.layers[3].get_weights())\n",
        "  quants = np.linspace(1, total, total)  \n",
        "  quants = np.power(quants, 2)\n",
        "  quants = quants / (total ** 2) * max_quantile\n",
        "  print(quants)\n",
        "  # return\n",
        "  for epoch, q in enumerate(quants):\n",
        "    train(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=10,\n",
        "        train_size=1024,\n",
        "        batch_size=16,\n",
        "        patch_size=256,\n",
        "        quant=q,\n",
        "        # loss_func=custom_loss(),\n",
        "        lr=0.0005,\n",
        "        lowest_lr=0.0002\n",
        "        )\n",
        "    # if q == quants[0]:   \n",
        "    #   sup_model.set_weights(model.get_weights())\n",
        "    # else:\n",
        "    #   for sup_layer, main_layer in zip(sup_model.layers, model.layers):\n",
        "    #     w_and_b_sup = sup_layer.get_weights()\n",
        "    #     w_and_b_sup_new = []\n",
        "    #     w_and_b_main = main_layer.get_weights()\n",
        "    #     for w_sup, w_main in zip(w_and_b_sup, w_and_b_main):\n",
        "    #       new_w = (w_sup * (total - 1) + w_main ) / total\n",
        "    #       w_and_b_sup_new.append(new_w)\n",
        "    #     sup_layer.set_weights(w_and_b_sup_new)\n",
        "\n",
        "    for x, y_true in test_dataset:\n",
        "      y_pred = model(x)\n",
        "      metric.update_state(y_true, y_pred)\n",
        "    print('model MAE = {:.5f}'.format(metric.result()))\n",
        "    metric.reset_states()\n",
        "    model.save_weights('/content/drive/MyDrive/Diploma/my_models/cur_model_epoch{}.h5'.format(epoch))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6omarMtaCQN1"
      },
      "source": [
        "x_test = None\n",
        "y_test = None\n",
        "\n",
        "\n",
        "# img4 = envi.open('/content/drive/MyDrive/Diploma/data_new/F15_20150101_20151231.v4.stable_lights.shiftx1y1.avg_vis.hdr', '/content/drive/MyDrive/Diploma/data_new/F15_20150101_20151231.v4.stable_lights.shiftx1y1.avg_vis')\n",
        "# img3 = envi.open('/content/drive/MyDrive/Diploma/data_new/VNL_v2_npp_2015_global_vcmslcfg_c202102150000.average_masked_resampled.hdr', '/content/drive/MyDrive/Diploma/data_new/VNL_v2_npp_2015_global_vcmslcfg_c202102150000.average_masked_resampled')\n",
        "# x_test = img3.load()\n",
        "# y_test = img4.load()\n",
        "# clip_value = 2000\n",
        "# x_test = np.clip(x_test, 0, clip_value)\n",
        "# x_test /= x_test.max()\n",
        "# y_test /= y_test.max()\n",
        "# pred_num = 4\n",
        "\n",
        "\n",
        "if x_test is None:\n",
        "  x_test = x_train\n",
        "  y_test = y_train\n",
        "  pred_num = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbfqE0ss_U6s"
      },
      "source": [
        "# model = perform_training(x_test, y_test, total=10, min_quantile=0.001, max_quantile=0.7, test_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtgLxZqXOFXt"
      },
      "source": [
        "# train(x_train, y_train, epochs=1, train_size=100, batch_size=12, patch_size=256, quant=0.002, loss_func=custom_loss())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EAN9iZ8n4np"
      },
      "source": [
        "# patch_size = 256\n",
        "# test_size = 5\n",
        "# gen_x, gen_y = get_generator(x_train, y_train, 0.9, patch_size=patch_size, count=test_size)\n",
        "# x_test_dataset = tf.data.Dataset.from_generator(gen_x, output_signature=tf.TensorSpec(shape=(patch_size, patch_size, 1), dtype=tf.float32))\n",
        "# y_test_dataset = tf.data.Dataset.from_generator(gen_y, output_signature=tf.TensorSpec(shape=(patch_size, patch_size, 1), dtype=tf.float32))\n",
        "# test_dataset = tf.data.Dataset.zip((x_test_dataset, y_test_dataset)).batch(1)\n",
        "# for x, y_true in test_dataset:\n",
        "#       y_pred = model(x)\n",
        "#       imshow(x.numpy())\n",
        "#       imshow(y_pred.numpy())\n",
        "#       imshow(y_true.numpy())\n",
        "#       loss = tf.reduce_mean(tf.keras.losses.mae(y_true, y_pred))\n",
        "#       print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak6yyJvMy7fg"
      },
      "source": [
        "model.compile()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO4CAlkHsk0I"
      },
      "source": [
        "model.save('/content/drive/MyDrive/Diploma/my_models/model_MAE_depth_4_per_level_3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJwBYTTJTV0V"
      },
      "source": [
        "# x_train = x_train[:-1, :-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Nso7zeDHqz"
      },
      "source": [
        "from skimage.util import view_as_windows, view_as_blocks\n",
        "\n",
        "def gen_restore_data(x_train_windows):\n",
        "  def gen():    \n",
        "    for i in range(x_train_windows.shape[0]):\n",
        "      for j in range(x_train_windows.shape[1]):\n",
        "        cnt = np.count_nonzero(x_train_windows[i, j, 0] > 0)\n",
        "        if cnt != 0:\n",
        "          yield x_train_windows[i, j, 0]\n",
        "  return gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLiRxAyYsOiF"
      },
      "source": [
        "from functools import wraps\n",
        "from time import time\n",
        "def measure(func):\n",
        "    @wraps(func)\n",
        "    def _time_it(*args, **kwargs):\n",
        "        start = int(round(time() * 1000))\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        finally:\n",
        "            end_ = int(round(time() * 1000)) - start\n",
        "            print(f\"Total execution time: {end_ if end_ > 0 else 0} ms\")\n",
        "    return _time_it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSbvxnuxdQ0a"
      },
      "source": [
        "# measure\n",
        "def get_window(a):\n",
        "  a = np.squeeze(a)\n",
        "  std = np.max(a.shape) / 4\n",
        "  center = (np.array((a.shape[0], a.shape[1]))[np.newaxis, ...] - 1) / 2\n",
        "  a = np.zeros_like(a)\n",
        "  ind = np.array(np.where(a == 0))\n",
        "  ind = np.moveaxis(ind, 0, -1).reshape((*a.shape, 2))\n",
        "  a = np.linalg.norm(ind - center, ord=2, axis=-1)\n",
        "  a = np.exp(-(np.power(a, 2) / (2 * std ** 2))) / (std * (2 * np.pi) ** (1 / 2))\n",
        "  return a / a.max()\n",
        "\n",
        "\n",
        "def perform_transfer(x, patch_size=256, step=64, cutoff=0):\n",
        "  orig_shape=x.shape\n",
        "  pad_values = (patch_size - 1 + step, patch_size - 1 + step)\n",
        "  x = np.pad(x, ((0, pad_values[0]), (0, pad_values[1]), (0,0)))\n",
        "  counter = np.zeros_like(x)\n",
        "  y = np.zeros_like(x)\n",
        "  y_coef = np.zeros_like(x)\n",
        "  window_coef = np.full((patch_size, patch_size, 1), 10E-5)\n",
        "  window_coef[..., 0] = get_window(window_coef[...])\n",
        "  x_train_windows = view_as_windows(x, (patch_size, patch_size, 1), step=step)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_generator(\n",
        "      gen_restore_data(x_train_windows),\n",
        "      output_signature=tf.TensorSpec(shape=(patch_size, patch_size, 1)))\n",
        "  dataset = dataset.batch(24)\n",
        "  try:\n",
        "    res = model.predict(dataset, verbose=0)\n",
        "  except ValueError:\n",
        "    return y[:orig_shape[0], :orig_shape[1]]\n",
        "  counter = 0\n",
        "  y_windows = view_as_windows(y, (patch_size, patch_size, 1), step=step)\n",
        "  y_coef_windows = view_as_windows(y_coef, (patch_size, patch_size, 1), step=step)\n",
        "  for i in range(y_windows.shape[0]):\n",
        "    for j in range(y_windows.shape[1]):\n",
        "      cnt = np.count_nonzero(x_train_windows[i, j, 0] > 0)\n",
        "      if cnt != 0:\n",
        "        y_windows[i, j, 0] += res[counter] * window_coef\n",
        "        counter += 1\n",
        "      y_coef_windows[i, j, 0] += window_coef\n",
        "  # lol = np.ones_like(y)\n",
        "  # lol[...] = 0\n",
        "  y = y / (y_coef + 10E-5)\n",
        "  return y[:orig_shape[0], :orig_shape[1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2ZpLekq7YMF"
      },
      "source": [
        "from sys import getsizeof\n",
        "def split_and_predict(x, split=(8, 8, 1), overlap=1/3, cutoff=5):\n",
        "\n",
        "  step = np.array(x.shape) // np.array(split)\n",
        "  overlap = step * overlap\n",
        "  overlap = overlap.astype(int)\n",
        "  step = step.astype(int)\n",
        "  orig_shape=x.shape\n",
        "  pad_values = (step[0] + overlap[0], step[1] + overlap[1])\n",
        "  x = np.pad(x, ((cutoff, pad_values[0]), (cutoff, pad_values[1]), (0,0)))\n",
        "  y = np.zeros_like(x)\n",
        "  coefs = np.zeros_like(x)\n",
        "  i_s_l = [i * step[0] for i in range(split[0] + 2)]\n",
        "  # i_s_l.append(x.shape[0] - overlap)\n",
        "  j_s_l = [i * step[1] for i in range(split[1] + 2)]\n",
        "  # j_s_l.append(x.shape[1] - overlap)\n",
        "  i_s_r = [i + overlap[0] for i in i_s_l[1:]]\n",
        "  j_s_r = [i + overlap[1] for i in j_s_l[1:]]\n",
        "  window_coef = get_window(np.zeros_like(x[:i_s_r[0], :j_s_r[0]]))[..., np.newaxis]\n",
        "  window_aux = np.zeros_like(window_coef)\n",
        "  window_aux[cutoff:-cutoff, cutoff:-cutoff] = 1\n",
        "  np.multiply(window_coef, window_aux, out=window_coef)\n",
        "  window_aux = None\n",
        "  print(i_s_l, i_s_r, j_s_l, j_s_r)\n",
        "  print(getsizeof(step), getsizeof(coefs),getsizeof(x),getsizeof(y), getsizeof(window_coef))\n",
        "  # print(window_coef.shape)\n",
        "  # return\n",
        "  for i_l, i_r in tqdm(zip(i_s_l[:-1], i_s_r), total=len(i_s_r)):\n",
        "    for j_l, j_r in zip(j_s_l[:-1], j_s_r):\n",
        "      temp = perform_transfer(x[i_l: i_r, j_l: j_r])\n",
        "      np.multiply(temp, window_coef, out=temp)\n",
        "      np.add(y[i_l: i_r, j_l: j_r], temp, out=y[i_l: i_r, j_l: j_r])\n",
        "      np.add(coefs[i_l: i_r, j_l: j_r], window_coef, coefs[i_l: i_r, j_l: j_r])\n",
        "      # y[i_l: i_r, j_l: j_r] += perform_transfer(x[i_l: i_r, j_l: j_r]) * window_coef\n",
        "      # coefs[i_l: i_r, j_l: j_r] += window_coef\n",
        "      # print(i_l, i_r, j_l, j_r)\n",
        "  np.add(coefs, 10E-5, out=coefs)\n",
        "  np.divide(y, coefs, out=y)\n",
        "  return y[cutoff:orig_shape[0] + cutoff, cutoff:orig_shape[1] + cutoff]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHgPj1t1Sd5o"
      },
      "source": [
        "x_train = y_train = y_test = None\n",
        "res = split_and_predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU0pnG_9Yh2c"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDlAjq2QRQo7"
      },
      "source": [
        "# x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eawZl56tmOej"
      },
      "source": [
        "res = res * 63\n",
        "res = np.around(res, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl2WqCbijQ5h"
      },
      "source": [
        "res.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sK6cYp_mV-U"
      },
      "source": [
        "y = 4983\n",
        "x = 7621\n",
        "imshow(res[y-256:y, x-256:x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73CC7n9bXimi"
      },
      "source": [
        "y = 1000\n",
        "x = 42000\n",
        "imshow(res[y-210:y, x-300:x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0e1lYeaVZeJ"
      },
      "source": [
        "np.savez('/content/drive/MyDrive/Diploma/Predictions/pred{}.npz'.format(pred_num), res[:,:,0])\n",
        "from tifffile import imsave\n",
        "# imsave('/content/drive/MyDrive/Diploma/preds_for_years/pred_{}.tiff'.format(2015), res[:,:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o269es1xVrp1"
      },
      "source": [
        "imshow(res)\n",
        "# imshow(x_train[:2000, 40000:])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}